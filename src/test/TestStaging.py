import os
import sys
import logging
import unittest
import warnings
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql import SparkSession

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.insert(1, ROOT_DIR)

from etl import Staging


class TestStaging(unittest.TestCase):
    database_name = "movies_data_test"
    spark = None
    logger = None
    stg = None

    @classmethod
    def setUp(self):
        warnings.filterwarnings("ignore", category=ResourceWarning)
        self.logger = logging.getLogger()
        console = logging.StreamHandler()
        console.setLevel(logging.DEBUG)
        console.setFormatter(logging.Formatter('[%(asctime)s] - %(levelname)s - %(message)s'))
        logging.getLogger('').addHandler(console)
        self.spark = SparkSession.builder.appName("Staging-Tests").getOrCreate()
        self.spark.sql("CREATE DATABASE IF NOT EXISTS {};".format(self.database_name))
        self.stg = Staging

    @classmethod
    def tearDownClass(self):
        self.spark.sql("DROP DATABASE {} CASCADE;".format(self.database_name))
        self.spark.stop()

    # test case to validate result returned by checkIfTableExists
    def test_check_if_table_exists(self):
        table_nm = "links"
        actual_output = self.stg.check_if_table_exists(self.logger, self.spark, self.database_name, table_nm)
        expected_output = False
        self.assertEqual(actual_output, expected_output)

    # test case to validate schema generated by stageRatingsFile
    def test_stage_ratings_files1(self):
        ratings_table_name = "ratings"
        ratings_update_table_name = "ratings_update"
        expected_schema = StructType([
            StructField('userId', LongType(), True),
            StructField('movieId', LongType(), True),
            StructField('rating', FloatType(), True),
            StructField('timestamp', TimestampType(), True),
            StructField('yearmo', IntegerType(), True)
        ])
        df1 = self.spark.createDataFrame(
            (["1", "10", "4.5", "964982703"], ["1", "11", "4.0", "998787858"], ["2", "11", "3.0", "901787858"]),
            ["userId", "movieId", "rating", "timestamp"])
        self.stg.stage_ratings_files(self.logger, self.spark, self.database_name, df1, ratings_table_name,
                                     ratings_update_table_name)
        df2 = self.spark.table("{}.{}".format(self.database_name, ratings_table_name))
        output_schema = df2.schema
        self.assertEqual(output_schema, expected_schema)

    # test case to validate data staged by stageRatingsFile
    def test_stage_ratings_files2(self):
        ratings_table_name = "ratings"
        ratings_update_table_name = "ratings_update"
        df1 = self.spark.createDataFrame(
            (["1", "10", "4.5", "964982703"], ["1", "11", "4.0", "998787858"], ["2", "11", "3.0", "901787858"]),
            ["userId", "movieId", "rating", "timestamp"])
        self.stg.stage_ratings_files(self.logger, self.spark, self.database_name, df1, ratings_table_name,
                                     ratings_update_table_name)
        expected_output = self.spark.createDataFrame(
            ([1, 10, 4.5, 964982703, 200007], [1, 11, 4.0, 998787858, 200108], [2, 11, 3.0, 901787858, 199807]),
            ["userId", "movieId", "rating", "timestamp"]).sort(
            "timestamp").collect()
        actual_output = self.spark.table("{}.{}".format(self.database_name, ratings_table_name)).withColumn("timestamp",
                                                                                                            unix_timestamp(
                                                                                                                col("timestamp"))).sort(
            "timestamp").collect()
        self.assertEqual(actual_output, expected_output)

    # test case to validate schema generated by stageMoviesFile
    def test_stage_movies_files1(self):
        movies_table_name = "movies"
        expected_schema = StructType([
            StructField('movieId', LongType(), True),
            StructField('title', StringType(), True),
            StructField('genres', StringType(), True)
        ])
        df1 = self.spark.createDataFrame(
            (["1", "alpha", "crime|thriller"], ["2", "beta", "crime"], ["2", "delta", "thriller"]),
            ["movieId", "title", "genres"])
        self.stg.stage_movies_files(self.logger, self.database_name, df1, movies_table_name)
        df2 = self.spark.table("{}.{}".format(self.database_name, movies_table_name))
        output_schema = df2.schema
        self.assertEqual(output_schema, expected_schema)

    # test case to validate data staged by stageMoviesFile
    def test_stage_movies_files2(self):
        movies_table_name = "movies"
        df1 = self.spark.createDataFrame(
            (["1", "alpha", "crime|thriller"], ["2", "beta", "crime"], ["2", "delta", "thriller"]),
            ["movieId", "title", "genres"])
        self.stg.stage_movies_files(self.logger, self.database_name, df1, movies_table_name)
        expected_output = self.spark.createDataFrame(
            ([1, "alpha", "crime|thriller"], [2, "beta", "crime"], [2, "delta", "thriller"]),
            ["movieId", "title", "genres"]).sort("movieId").collect()
        actual_output = self.spark.table("{}.{}".format(self.database_name, movies_table_name)).sort(
            "movieId").collect()
        self.assertEqual(actual_output, expected_output)

    # test case to validate schema generated by stageTagsFile
    def test_stage_tags_files1(self):
        tags_table_name = "tags"
        expected_schema = StructType([
            StructField('userId', LongType(), True),
            StructField('movieId', LongType(), True),
            StructField('tag', StringType(), True),
            StructField('timestamp', TimestampType(), True)
        ])

        df1 = self.spark.createDataFrame(
            (
                ["1", "10", "alpha", "964982703"], ["1", "11", "beta", "998787858"], ["2", "11", "delta", "901787858"]),
            ["userId", "movieId", "tag", "timestamp"])
        self.stg.stage_tags_files(self.logger, self.database_name, df1, tags_table_name)
        df2 = self.spark.table("{}.{}".format(self.database_name, tags_table_name))
        output_schema = df2.schema
        self.assertEqual(output_schema, expected_schema)

    # test case to validate data staged by stageTagsFile
    def test_stage_tags_files2(self):
        tags_table_name = "tags"
        ratings_update_table_name = "ratings_update"
        df1 = self.spark.createDataFrame(
            (
                ["1", "10", "alpha", "964982703"], ["1", "11", "beta", "998787858"], ["2", "11", "delta", "901787858"]),
            ["userId", "movieId", "tag", "timestamp"])
        self.stg.stage_tags_files(self.logger, self.database_name, df1, tags_table_name)
        expected_output = self.spark.createDataFrame(
            ([1, 10, "alpha", 964982703], [1, 11, "beta", 998787858], [2, 11, "delta", 901787858]),
            ["userId", "movieId", "tag", "timestamp"]).sort("timestamp").collect()
        actual_output = self.spark.table("{}.{}".format(self.database_name, tags_table_name)).sort(
            "timestamp").withColumn("timestamp", unix_timestamp(col("timestamp"))).collect()
        self.assertEqual(actual_output, expected_output)


if __name__ == "__main__":
    unittest.main()
